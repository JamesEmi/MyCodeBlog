<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="James Emilian">
<meta name="dcterms.date" content="2022-10-31">

<title>Text prompt-based Image Masking using Stable Diffusion: A series of experiments – JCodes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5e969457915964b552036b6e35ee7f03.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">JCodes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/JamesEmi"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/JamesEmilian2"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Text prompt-based Image Masking using Stable Diffusion: A series of experiments</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Code</div>
                <div class="quarto-category">AI</div>
                <div class="quarto-category">StableDiffusion</div>
                <div class="quarto-category">fast.ai</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>James Emilian </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 31, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction-setup" class="level2">
<h2 class="anchored" data-anchor-id="introduction-setup">Introduction &amp; Setup</h2>
<p>Inspired by the amazing paper “<a href="https://arxiv.org/abs/2210.11427">DiffEdit: Diffusion-based semantic image editing with mask guidance</a>” which <a href="https://twitter.com/jeremyphoward">Jeremy</a> discussed in last week’s class of <a href="https://www.fast.ai/posts/part2-2022.html">Practical Deep Learning for Coders Pt2</a>, this is an attempt (v1) at implementing step one of the paper’s semantic image editing method.</p>
<p>See below an image from the paper, illustrating this novel method of editing images simply using a text query:</p>
<p><img src="DiffEditDemo.png" class="img-fluid"></p>
<p>Seeing the automatic mask generation was fascinating for me, and it was evident from interactions during the class that this was indeed a novel, interesting way to generate pixel masks. Stable Diffusion, the poster-child for modern AI, and the basis for cool tech like <a href="https://beta.dreamstudio.ai/">Dream Studio</a> and <a href="https://www.midjourney.com/">Midjourney</a>, could do something more pragmatic too! The mask generation strategy is simple - it involves using stable diffusion to create a pixel mask for an input image, simply using two text prompts. Instead of going off on an verbose explanation I’ll allow a screenshot from the DiffEdit paper to do the job. <!-- *I'm yet to implement step 2 & step 3; so we'll save that for another post* --></p>
<p><img src="DiffEditStep1.png" class="img-fluid"></p>
<p><em>If you wish to delve deeper into Stable Diffusion I highly recommend going through this amazing repo from fast.ai - <a href="https://github.com/fastai/diffusion-nbs">diffusion nbs on GitHub</a>.</em></p>
<p>To quote the paper, for the mask creation - <em>“We add noise to the input image, and denoise it: once conditioned on the query text, and once conditioned on a reference text (or unconditionally). We derive a mask based on the difference in the denoising results”</em>. The above screenshot is part of a detailed schematic - where the input is the image x<sub>0</sub> and query text <code>Q = 'Zebra'</code> and reference text <code>R = 'Horse'</code>. The <code>StableDiffusionImg2ImgPipeline</code> pipeline from HuggingFace takes care of adding the Gaussian noise to an input image, and denoising the resulting image by running inference using a pretrained model. We can play around with the inference input parameters like input image, number of inference steps, strength of prompt guidance, etc. to influence the denoised image output. <br> <em>Note: This post documents multiple experiments, with varying levels of quality of final output. The intent is to capture the thought process and important trial and error and resulting insights that went into trying to generate a good pixel mask using SD</em>.</p>
<div id="87c1d9c9-6a67-425f-8b8e-6ddb8588be79" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> diffusers <span class="im">import</span> StableDiffusionImg2ImgPipeline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Load the pretrained <code>StableDiffusionImg2ImgPipeline</code> onto <code>pipe</code>:</p>
<div id="6f1a8498-64a6-4ed3-b55f-cb5edb7b3e63" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> StableDiffusionImg2ImgPipeline.from_pretrained(<span class="st">"CompVis/stable-diffusion-v1-4"</span>,revision<span class="op">=</span><span class="st">"fp16"</span>,torch_dtype<span class="op">=</span>torch.float16, safety_checker <span class="op">=</span> <span class="va">None</span>).to(<span class="st">"cuda"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s load and take a look at the input image, x<sub>0</sub> :</p>
<div id="855c50e4-2683-4b48-80b2-b5abdb330d1d" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>init_image <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">"dark-horse-riverV2.jpeg"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>pixels <span class="op">=</span> init_image.load()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>init_image.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="single-output---based-mask-generation" class="level2">
<h2 class="anchored" data-anchor-id="single-output---based-mask-generation">Single output - based mask generation</h2>
<p>In this case, we’ll keep <code>num_images_per_prompt=1</code> in the image-image stable diffusion pipeline, and see how good a mask we can generate using that. We’ll play around a bit with <code>num_inference_steps</code> to create the best possible mask.</p>
<p><code>pipe</code> takes care of adding noise to the input image <code>init_image</code>, and denoising it for <code>num_inference_steps=8</code>, with respect to the reference text - <code>"a horse"</code>.</p>
<div id="4c07afd4-6514-40ee-87ae-07846016fa14" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse"</span> <span class="co">#Denoising image with respect to the identifying phrase </span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">10</span>).images</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>image[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b12ba323a8ab4708b41fcccec24ee74d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="12">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-5-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>That’s a scary looking horse, one must admit! For some reason I’ve seen from my experiments (not recorded here) that the model often grows extra appendages from the tail pixels of horse images. We can try reducing <code>num_inference_steps</code> to prevent that, but the downside is that the output image will be noisy, since we haven’t allowed for enough inference steps. For now, we’ll move on and see what happens with the query text denoising.</p>
<p>We use pipe to add noise to the input image init_image, and denoise it for <code>num_inference_steps=10</code>, with respect to the query text - <code>"a zebra"</code>.</p>
<div id="b0ae3294-fb48-4952-8600-06f9fc0c2b2e" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>image_q <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">10</span>).images</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>image_q[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"74fded865f6147efa49ddac3cd58ceaf","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="14">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-6-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>That was unexpected! Let’s reduce <code>num_inference_steps</code> to <code>8</code> to prevent the inference from going too far!</p>
<div id="c1aa948b-4baf-4b1a-8fb3-f8cccaf504e4" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>image_q <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">8</span>).images</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>image_q[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c6987c5941154830a8b43e7276506af9","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="15">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-7-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Better. Notice how the background is changing considerably based on the prompt. Let’s try altering the prompt to make sure the backgrounds are relatively similar in the images denoised with respect to both Query Q and Reference R.</p>
<div id="5237fe6e-ef5f-443f-8832-1051342a8ffb" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">10</span>).images</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>image[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4c7b241a38604e84abbca96cff80d3a3","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="20">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-8-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="2aa81888-08a0-45d3-b88e-6a3a81956462" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>image_q <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">10</span>).images</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>image_q[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"18ef9c75896f4b7e94f4b65c53952cf0","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="21">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Notice how giving more information in the prompt about the action of the subject with the input “a zebra drinking water” has allowed the model to run for 10 inference steps without morphing the zebra too badly!</p>
<p>Neither of those are particularly pretty images on their own; but let’s attempt to find the “normalised difference between the denoised images”, and generate a rough mask. Then we can try to play with <code>num_inference_steps</code> and create a better mask. Once that parameter’s ideal value is clear we can move to trying to use multiple denoised images.</p>
<div id="65132e17-7879-4c8d-a237-87fbe19a4623" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>convert_to_tensor <span class="op">=</span> transforms.ToTensor()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>convert_to_image <span class="op">=</span> transforms.ToPILImage()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since we’ll be using this operation often, let’s define a function to compute the normalised absolute difference between two images.</p>
<div id="736fbfde-7ccb-4a2e-94eb-dce479e65726" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> norm_diff_abs(a,b):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    a_tens <span class="op">=</span> convert_to_tensor(a)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    b_tens <span class="op">=</span> convert_to_tensor(b)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    ftens <span class="op">=</span> (<span class="bu">abs</span>(a_tens<span class="op">-</span>b_tens))<span class="op">/</span>(a_tens<span class="op">+</span>b_tens)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    fimage <span class="op">=</span> convert_to_image(ftens)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fimage</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that <code>abs()</code> is an important part of getting a meaningful normalised difference image. Without it the resulting difference image doesn’t serve our purpose well. Ponder the reason, dear reader ;)</p>
<div id="f49c0c9a-4719-40f9-9fc8-82b360cd20f0" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs <span class="op">=</span> norm_diff_abs(image_q[<span class="dv">0</span>], image[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="24f5c9f6-7018-4542-81c5-157deefc2b98" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now that we have the normalised difference between the denoised images (a visual representation of the contrast between reference text and query), let’s write a function to convert this image to grayscale and binarize this. This function, when called, should yield the final mask <code>M</code>.</p>
<div id="3d64ac25-9bdf-40f1-9870-6cb7a508dc0d" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> GSandBinarize(im, <span class="op">**</span>kwargs):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    im_array <span class="op">=</span> asarray(im)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    im_grayscale <span class="op">=</span> cv2.cvtColor(im_array, cv2.COLOR_BGR2GRAY)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kwargs[<span class="st">"thresh_method"</span>] <span class="op">==</span> <span class="st">'manual'</span>:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        th, im_binary_array <span class="op">=</span> cv2.threshold(im_grayscale, kwargs[<span class="st">"thresh"</span>], <span class="dv">255</span>, cv2.THRESH_BINARY) <span class="co">#manual thresholding</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        th, im_binary_array <span class="op">=</span> cv2.threshold(im_grayscale, <span class="dv">0</span>, <span class="dv">255</span>, cv2.THRESH_OTSU) <span class="co">#auto thresholding</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    im_binary <span class="op">=</span> Image.fromarray(im_binary_array)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> im_binary      </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s try binarising the difference using the OTSU thresholding method, which will automatically pick a pixel threshold value.</p>
<div id="5ca093a3-5cd9-4b37-a123-71de0891c1b9" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>GSandBinarize(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The backgound is not too shabby; it’s almost fully blacked out, but the horse body has a lot of black areas. Now let’s try binarising the difference using the binary thresholding method, which allows us to pick a pixel threshold value.</p>
<div id="6e467a14-1890-4575-9f9d-3f787a8e80e6" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now, in this case, more of the horse’s body is white, making the mask better at capturing the subject; but lowering the pixel threshold manually has given rise to many white patches in the background as well. Let’s try solving this.</p>
<p><em>SideNote: For the exact same model, random seed, and parameters, the mask seems to come out slightly better on a A4000 GPU, as compared to a P5000. All outputs shown here are from running inference on a Paperspace A4000 instance.</em></p>
<p>Now that we have the whole mask generation process ready, let’s play a bit with the <code>num_inference_steps</code> parameter to see if we can get a better mask!! Let’s start with <code>num_inference_steps=6</code>.</p>
<div id="2df0f2ad-44de-4507-a450-7d9697e42556" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase (lower number of inf steps)</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">6</span>).images</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>image[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e9071831310549b496214f27db762533","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="32">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-17-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="92cfbe89-dab1-4f7e-a067-edef1e626b7b" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase (lower number of inf steps)</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>image_q <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">6</span>).images</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>image_q[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"904d4412615b46518ad46f4863a089d1","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="33">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-18-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="76aa7af4-f9e0-47cf-9367-225b7cc87d34" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs <span class="op">=</span> norm_diff_abs(image_q[<span class="dv">0</span>], image[<span class="dv">0</span>])</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="91b97d13-73bb-4c2d-b398-6a7a51b033ec" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As we can see, this leads to a much clearer outline of the horse than before, but the image is littered with too many white patches. I suspect that that can be solved by averaging out multiple outputs (see next section), and so <code>num_inference_steps=6</code> can be considered a good candidate for our final run. Now let’s try <code>num_inference_steps=8</code>:</p>
<div id="f9710559-f887-41b6-9930-0573f13be9db" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">8</span>).images</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>image[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"57a89009c58f4041b6662374e23ac4c4","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="36">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-21-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="f1abecff-0dda-4507-bc7c-bc1306aef704" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>image_q <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">8</span>).images</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>image_q[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"00e85c64f96d436697cb031879b7254d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="37">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-22-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="4f8a55d3-2987-40bb-a655-649b6e8b0094" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs <span class="op">=</span> norm_diff_abs(image_q[<span class="dv">0</span>], image[<span class="dv">0</span>])</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="0beae82b-b71f-4f7d-9ee5-44eb7011b617" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Well why not use a higher <code>num_inference_steps</code> to allow for more denoising? Fair point; but what often seems to happen is that, weird artifacts are generated at a high value of <code>num_inference_steps</code> :</p>
<div id="454e8d55-002e-4350-9692-abab59aa995c" class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>image_q <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">30</span>).images</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>image_q[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0ece78f5aa7b431a917622cb47c666fe","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="74">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-25-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>A super clean-looking image, but now the zebra has two heads!</p>
<div id="5505b02f-5490-4b89-999b-40a2b7e08e33" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">30</span>).images</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>image[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"54c5e28ee6a547de9dda0a1f3c1971d9","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="43">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-26-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="40f59a1b-6ed5-4c5c-9748-38ffd2a932ac" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs <span class="op">=</span> norm_diff_abs(image_q[<span class="dv">0</span>], image[<span class="dv">0</span>])</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="1bf40b98-10ea-41be-96a0-3075f7af5ceb" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-28-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It’s still just as noisy as the mask outputs we got using a much lower value of <code>num_inference_steps</code>, and now, the outline has become less accurate because of all the extra heads in the denoised images! Let’s stick to a lower value for <code>num_inference_steps</code> and come up with a different way to get a clearer mask.</p>
</section>
<section id="multi-output---based-mask-generation" class="level2">
<h2 class="anchored" data-anchor-id="multi-output---based-mask-generation">Multi output - based mask generation</h2>
<p>Since we are denoising a image after adding Gaussian noise to it, we could try to generate multiple denoised images for each prompt, and average them out before taking the normalised difference of query versus reference. The idea is that averaging multiple images denoised based on the same prompt will help cancel out the random noise, and also create a clearer image of the model’s idea of a “zebra” or “horse”. <br> <em>Note: Judging from the DiffEdit paper’s schematic of the process, this is likely the method adopted by its authors.</em></p>
<div id="bf9dd9e0-327a-4654-8310-0b015370fd9b" class="cell" data-execution_count="108">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>pipe.enable_attention_slicing()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="db3576b1-2dc7-41a9-a3bb-0b387de9fd05" class="cell" data-execution_count="164">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dark-horse-riverv2-rsz</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>init_image <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">"dark-horse-riverV2-rsz.jpg"</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>pixels <span class="op">=</span> init_image.load()</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>init_image.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><em>The image has been cropped and resized to avoid a CUDA OOM error when running for a high value of</em> <code>num_images_per_prompt</code>.</p>
<p>First, we try to implement this with 10 images per prompt, denoised for <code>num_inference_steps=6</code>.</p>
<div id="cc2e7d31-3abe-4b83-bc42-2bcb6a9f45fb" class="cell" data-execution_count="109">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>images_r6 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">6</span>).images</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_r6, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"779c267c6949488ba54b5c47541485ad","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="109">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-31-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="7bbb7041-df00-4c89-97e8-2ae24c687bad" class="cell" data-execution_count="110">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>images_q6 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">6</span>).images</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_q6, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"36b74dc74e324a649eefebd4018605d2","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="110">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-32-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let’s build out the function for passing in the list of images; and getting the averaged out image! How? Stack the list of images generated and create an average of them, along the “number of images” axis. <br> <em>Note - This function, like all others, was built by experimenting with each line of its components and then putting them together - a nifty trick, courtesy of Jeremy!</em></p>
<div id="9bedbcab-3e21-4b85-a8d7-7b4f60ce2c57" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_averageIm(ImList):</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    imtensors <span class="op">=</span> []</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> Im <span class="kw">in</span> ImList:</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>        imtensor <span class="op">=</span> convert_to_tensor(Im)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>        imtensors.append(imtensor)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    init_tensor <span class="op">=</span> torch.zeros(imtensors[<span class="dv">0</span>].shape)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tensor <span class="kw">in</span> imtensors:</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        init_tensor <span class="op">+=</span> tensor</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    total_tensor <span class="op">=</span> init_tensor</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    av_imtensor <span class="op">=</span> (total_tensor)<span class="op">/</span><span class="dv">10</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    av_image <span class="op">=</span> convert_to_image(av_imtensor)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> av_image</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="ab2adc96-3c8f-4dbc-888a-3356474d5dd9" class="cell" data-execution_count="111">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>av_image_r6 <span class="op">=</span> get_averageIm(images_r6)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>av_image_r6</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="111">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-34-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="ed89be23-c707-4bf6-8ed5-7532a2c85541" class="cell" data-execution_count="112">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>av_image_q6 <span class="op">=</span> get_averageIm(images_q6)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>av_image_q6</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="112">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-35-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Like it or not, this is what peak “horse” and “zebra” performance look like! (reference - <a href="https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb">Chapter 4 of fastbook</a>).</p>
<div id="dd1f5029-d8f1-4a68-b93e-aeca3bee5282" class="cell" data-execution_count="113">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs6 <span class="op">=</span> norm_diff_abs(av_image_r6, av_image_q6)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs6, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="113">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-36-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="8e5254dc-c6d2-4518-b332-a87f1aa98556" class="cell" data-execution_count="115">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>mask_6step <span class="op">=</span> GSandBinarise(im_diff_normabs6, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>mask_6step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="115">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-37-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Repeat the same for other <code>num_inference_steps=8</code> values to get a better mask. Also remember to play around with manual threshold value, to get ideal mask. <br></p>
<p>The mask output looked decent for <code>num_inference_steps=8</code> in the single-output section, so let’s try that here:</p>
<div id="9aa74a6c-d97c-4658-80e5-0ee46ca6dd86" class="cell" data-execution_count="116">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>images_r8 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">8</span>).images</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_r8, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"76dac886e1c74328b39fd6d77c39d97e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="116">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-38-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="2e6d4e15-029d-4f39-a6c9-ab9faa382724" class="cell" data-execution_count="117">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>images_q8 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">8</span>).images</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_q8, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e17346df6572475698f9afb835e00fa1","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="117">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-39-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="5a6b09a9-cb97-4f89-81b9-ac5dfc84f929" class="cell" data-execution_count="118">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>av_image_r8 <span class="op">=</span> get_averageIm(images_r8)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>av_image_r8</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="118">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-40-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="c83b32af-f2d9-47c7-8aca-05e3a61fd8bf" class="cell" data-execution_count="119">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>av_image_q8 <span class="op">=</span> get_averageIm(images_q8)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>av_image_q8</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="119">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-41-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="9371ad22-0138-4424-a861-643d0f608ee9" class="cell" data-execution_count="120">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs8 <span class="op">=</span> norm_diff_abs(av_image_r8, av_image_q8)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>mask_otsu_8step <span class="op">=</span> GSandBinarise(im_diff_normabs8, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>mask_otsu_8step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="120">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-42-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="0c308c18-9c77-4f33-87c3-667ade116b4f" class="cell" data-execution_count="123">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>mask_8step <span class="op">=</span> GSandBinarise(im_diff_normabs8, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>mask_8step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="123">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-43-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>That looks better than the previous mask. We’ve considerably reduced the white patches in the background. Let’s try <code>num_inference_steps=7</code>.</p>
<div id="f3212692-116b-49ac-b6e3-69e63db42bfb" class="cell" data-execution_count="125">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co">#inf steps 7</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>images_r7 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">7</span>).images</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>image_grid(images_r7, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"397fc3f58a744b90b120d0652a623be8","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="125">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-44-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="5d4280ab-b2b6-428b-9b31-26e9183bc637" class="cell" data-execution_count="126">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>images_q7 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">7</span>).images</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_q7, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9c8868ca16f9462fa6dd6f11e7c35a96","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="126">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-45-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="b7c8e85b-d0ca-4416-9186-ba1b90b0d49d" class="cell" data-execution_count="129">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>av_image_r7 <span class="op">=</span> get_averageIm(images_r7)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>av_image_r7</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="129">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-46-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="4d317683-089a-4d63-8021-8890ba5d4820" class="cell" data-execution_count="130">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>av_image_q7 <span class="op">=</span> get_averageIm(images_q7)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>av_image_q7</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="130">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-47-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="a4226454-bac8-404c-a18b-13105fa7d51a" class="cell" data-execution_count="131">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs7 <span class="op">=</span> norm_diff_abs(av_image_r7, av_image_q7)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>mask_otsu_7step <span class="op">=</span> GSandBinarise(im_diff_normabs7, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>mask_otsu_7step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="131">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-48-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="5ed987c2-7b5a-45c5-813f-9b81c803da23" class="cell" data-execution_count="132">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>mask_7step <span class="op">=</span> GSandBinarise(im_diff_normabs7, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>mask_7step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="132">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-49-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This mask (<code>num_inference_steps=7</code>) has a slightly cleaner outline than the one for <code>num_inference_steps=8</code>. Let’s try going a bit higher; <code>num_inference_steps=10</code></p>
<div id="d0c74d75-f03d-4e57-a798-89ad1d06d812" class="cell" data-execution_count="133">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>images_r10 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">10</span>).images</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_r10, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"765870237c4a49228b7a3b3a1699385e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="133">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-50-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="f9325ea8-1ef2-4374-968c-b6f31f01b3bd" class="cell" data-execution_count="134">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>images_q10 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">10</span>).images</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_q10, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"87e5dfaae6ff4ccbbbc77b74294019f4","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="134">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-51-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As one can see, there are a few weird images, and all the images are quite deformed with respect to the original <code>init_image</code>. Let’s look at the generated mask.</p>
<div id="336c4a08-8e22-41f1-8bc0-17e62c5a851e" class="cell" data-execution_count="135">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>av_image_r10 <span class="op">=</span> get_averageIm(images_r10)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>av_image_r10</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>av_image_q10 <span class="op">=</span> get_averageIm(images_q10)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>av_image_q10</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="135">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-52-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="e7b72fca-c4e6-4941-94d6-68bae37f3572" class="cell" data-execution_count="137">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs10 <span class="op">=</span> norm_diff_abs(av_image_r10, av_image_q10)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>mask_otsu_10step <span class="op">=</span> GSandBinarise(im_diff_normabs10, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>mask_otsu_10step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="137">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-53-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="8992de3a-2bf7-4e76-8793-e9824c6f5f88" class="cell" data-execution_count="138">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>mask_10step <span class="op">=</span> GSandBinarise(im_diff_normabs10, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>mask_10step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="138">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-54-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We start to see a more patchy outline, because the denoised images have deviated more and more from the original outline. Let’s go down one step and try <code>num_inference_steps=9</code>.</p>
<div id="1bf2530e-1bb6-4843-a305-c3943dca80ab" class="cell" data-execution_count="142">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>images_r9 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">9</span>).images</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_r9, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"17ead7d5880b4e4f81d465d1f067cea2","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="142">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-55-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="eab9d064-f91e-408a-b60d-742aa10b9dfc" class="cell" data-execution_count="139">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>images_q9 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">9</span>).images</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_q9, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"38542981fc124410840d767beb5bf778","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="139">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-56-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The images are still weird and vary considerably in the position of the subject, so we can expect the mask to be “noisy” as well.</p>
<div id="8f59ba1c-e4e2-41e2-9080-7ea630feaf90" class="cell" data-execution_count="143">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>av_image_r9 <span class="op">=</span> get_averageIm(images_r9)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>av_image_r9</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="143">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-57-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="ea720cdb-fa89-4d0d-9f27-40484f127b5e" class="cell" data-execution_count="144">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>av_image_q9 <span class="op">=</span> get_averageIm(images_q9)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>av_image_q9</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="144">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-58-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="79884359-511a-419b-bee9-77ffa70f6b67" class="cell" data-execution_count="145">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs9 <span class="op">=</span> norm_diff_abs(av_image_r9, av_image_q9)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>mask_otsu_9step <span class="op">=</span> GSandBinarise(im_diff_normabs9, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>mask_otsu_9step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="145">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-59-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="a543dac0-a0d8-4541-a37b-a169d26307b3" class="cell" data-execution_count="146">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>mask_9step <span class="op">=</span> GSandBinarise(im_diff_normabs9, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>mask_9step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="146">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-60-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Though better than the previous run, this still has a rather unclear outline. From the above analyses, we can pick <code>num_inference_steps=6, 7, 8</code> as yielding usable pixel masks.</p>
<div id="1a3e2a21-f231-4ec1-a273-c068b2bf38d1" class="cell" data-execution_count="168">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>mask_list <span class="op">=</span> [mask_6step, mask_7step, mask_8step]</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>image_grid(mask_list, rows<span class="op">=</span><span class="dv">1</span>, cols<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="168">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-61-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>If we could see how these masks look on the original image, I suppose they’d be more convincing. I’m currently facing a minor bug in overlaying a red mask onto the original image; and this notebook will be updated with the same once that is resolved.</p>
<p>Though a far cry from the clean mask illustrated in the DiffEdit paper, I suppose this isn’t a bad start. As I experiment with other techniques (like using other noise schedulers) to generate a fully clean mask, I’ll be adding relevant updates on this post. <br></p>
<p>Hopefully you got something from your time here! If you read this and have any suggestions/comments on how to improve my code (or words!), please reach out to me <a href="https://twitter.com/jamesemilian2"><span class="citation" data-cites="Twitter">@Twitter</span></a>. <br></p>
<p>Cheers!! 😄</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>