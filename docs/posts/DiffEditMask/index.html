<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.253">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="James Emilian">
<meta name="dcterms.date" content="2022-10-31">

<title>JCodes - Text prompt-based Image Masking using Stable Diffusion: A series of experiments</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">JCodes</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/JamesEmi"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/JamesEmilian2"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Text prompt-based Image Masking using Stable Diffusion: A series of experiments</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Code</div>
                <div class="quarto-category">AI</div>
                <div class="quarto-category">StableDiffusion</div>
                <div class="quarto-category">fast.ai</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>James Emilian </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 31, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction-setup" class="level2">
<h2 class="anchored" data-anchor-id="introduction-setup">Introduction &amp; Setup</h2>
<p>Inspired by the amazing paper “<a href="https://arxiv.org/abs/2210.11427">DiffEdit: Diffusion-based semantic image editing with mask guidance</a>” which <a href="https://twitter.com/jeremyphoward">Jeremy</a> discussed in last week’s class of <a href="https://www.fast.ai/posts/part2-2022.html">Practical Deep Learning for Coders Pt2</a>, this is an attempt (v1) at implementing step one of the paper’s semantic image editing method.</p>
<p>See below an image from the paper, illustrating this novel method of editing images simply using a text query:</p>
<p><img src="DiffEditDemo.png" class="img-fluid"></p>
<p>Seeing the automatic mask generation was fascinating for me, and it was evident from interactions during the class that this was indeed a novel, interesting way to generate pixel masks. Stable Diffusion, the poster-child for modern AI, and the basis for cool tech like <a href="https://beta.dreamstudio.ai/">Dream Studio</a> and <a href="https://www.midjourney.com/">Midjourney</a>, could do something more pragmatic too! The mask generation strategy is simple - it involves using stable diffusion to create a pixel mask for an input image, simply using two text prompts. Instead of going off on an verbose explanation I’ll allow a screenshot from the DiffEdit paper to do the job. <!-- *I'm yet to implement step 2 & step 3; so we'll save that for another post* --></p>
<p><img src="DiffEditStep1.png" class="img-fluid"></p>
<p><em>If you wish to delve deeper into Stable Diffusion I highly recommend going through this amazing repo from fast.ai - <a href="https://github.com/fastai/diffusion-nbs">diffusion nbs on GitHub</a>.</em></p>
<p>To quote the paper, for the mask creation - <em>“We add noise to the input image, and denoise it: once conditioned on the query text, and once conditioned on a reference text (or unconditionally). We derive a mask based on the difference in the denoising results”</em>. The above screenshot is part of a detailed schematic - where the input is the image x<sub>0</sub> and query text <code>Q = 'Zebra'</code> and reference text <code>R = 'Horse'</code>. The <code>StableDiffusionImg2ImgPipeline</code> pipeline from HuggingFace takes care of adding the Gaussian noise to an input image, and denoising the resulting image by running inference using a pretrained model. We can play around with the inference input parameters like input image, number of inference steps, strength of prompt guidance, etc. to influence the denoised image output. <br> <em>Note: This post documents multiple experiments, with varying levels of quality of final output. The intent is to capture the thought process and important trial and error and resulting insights that went into trying to generate a good pixel mask using SD</em>.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> diffusers <span class="im">import</span> StableDiffusionImg2ImgPipeline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Load the pretrained <code>StableDiffusionImg2ImgPipeline</code> onto <code>pipe</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> StableDiffusionImg2ImgPipeline.from_pretrained(<span class="st">"CompVis/stable-diffusion-v1-4"</span>,revision<span class="op">=</span><span class="st">"fp16"</span>,torch_dtype<span class="op">=</span>torch.float16, safety_checker <span class="op">=</span> <span class="va">None</span>).to(<span class="st">"cuda"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s load and take a look at the input image, x<sub>0</sub> :</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>init_image <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">"dark-horse-riverV2.jpeg"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>pixels <span class="op">=</span> init_image.load()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>init_image.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="single-output---based-mask-generation" class="level2">
<h2 class="anchored" data-anchor-id="single-output---based-mask-generation">Single output - based mask generation</h2>
<p>In this case, we’ll keep <code>num_images_per_prompt=1</code> in the image-image stable diffusion pipeline, and see how good a mask we can generate using that. We’ll play around a bit with <code>num_inference_steps</code> to create the best possible mask.</p>
<p><code>pipe</code> takes care of adding noise to the input image <code>init_image</code>, and denoising it for <code>num_inference_steps=8</code>, with respect to the reference text - <code>"a horse"</code>.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse"</span> <span class="co">#Denoising image with respect to the identifying phrase </span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">10</span>).images</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>image[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b12ba323a8ab4708b41fcccec24ee74d","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="12">
<p><img src="index_files/figure-html/cell-5-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>That’s a scary looking horse, one must admit! For some reason I’ve seen from my experiments (not recorded here) that the model often grows extra appendages from the tail pixels of horse images. We can try reducing <code>num_inference_steps</code> to prevent that, but the downside is that the output image will be noisy, since we haven’t allowed for enough inference steps. For now, we’ll move on and see what happens with the query text denoising.</p>
<p>We use pipe to add noise to the input image init_image, and denoise it for <code>num_inference_steps=10</code>, with respect to the query text - <code>"a zebra"</code>.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>image_q <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">10</span>).images</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>image_q[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"74fded865f6147efa49ddac3cd58ceaf","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="14">
<p><img src="index_files/figure-html/cell-6-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>That was unexpected! Let’s reduce <code>num_inference_steps</code> to <code>8</code> to prevent the inference from going too far!</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>image_q <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">8</span>).images</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>image_q[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c6987c5941154830a8b43e7276506af9","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="15">
<p><img src="index_files/figure-html/cell-7-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Better. Notice how the background is changing considerably based on the prompt. Let’s try altering the prompt to make sure the backgrounds are relatively similar in the images denoised with respect to both Query Q and Reference R.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">10</span>).images</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>image[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4c7b241a38604e84abbca96cff80d3a3","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="20">
<p><img src="index_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>image_q <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">10</span>).images</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>image_q[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"18ef9c75896f4b7e94f4b65c53952cf0","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="21">
<p><img src="index_files/figure-html/cell-9-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Notice how giving more information in the prompt about the action of the subject with the input “a zebra drinking water” has allowed the model to run for 10 inference steps without morphing the zebra too badly!</p>
<p>Neither of those are particularly pretty images on their own; but let’s attempt to find the “normalised difference between the denoised images”, and generate a rough mask. Then we can try to play with <code>num_inference_steps</code> and create a better mask. Once that parameter’s ideal value is clear we can move to trying to use multiple denoised images.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>convert_to_tensor <span class="op">=</span> transforms.ToTensor()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>convert_to_image <span class="op">=</span> transforms.ToPILImage()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since we’ll be using this operation often, let’s define a function to compute the normalised absolute difference between two images.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> norm_diff_abs(a,b):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    a_tens <span class="op">=</span> convert_to_tensor(a)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    b_tens <span class="op">=</span> convert_to_tensor(b)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    ftens <span class="op">=</span> (<span class="bu">abs</span>(a_tens<span class="op">-</span>b_tens))<span class="op">/</span>(a_tens<span class="op">+</span>b_tens)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    fimage <span class="op">=</span> convert_to_image(ftens)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fimage</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that <code>abs()</code> is an important part of getting a meaningful normalised difference image. Without it the resulting difference image doesn’t serve our purpose well. Ponder the reason, dear reader ;)</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs <span class="op">=</span> norm_diff_abs(image_q[<span class="dv">0</span>], image[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<p><img src="index_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Now that we have the normalised difference between the denoised images (a visual representation of the contrast between reference text and query), let’s write a function to convert this image to grayscale and binarize this. This function, when called, should yield the final mask <code>M</code>.</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> GSandBinarize(im, <span class="op">**</span>kwargs):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    im_array <span class="op">=</span> asarray(im)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    im_grayscale <span class="op">=</span> cv2.cvtColor(im_array, cv2.COLOR_BGR2GRAY)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kwargs[<span class="st">"thresh_method"</span>] <span class="op">==</span> <span class="st">'manual'</span>:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        th, im_binary_array <span class="op">=</span> cv2.threshold(im_grayscale, kwargs[<span class="st">"thresh"</span>], <span class="dv">255</span>, cv2.THRESH_BINARY) <span class="co">#manual thresholding</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        th, im_binary_array <span class="op">=</span> cv2.threshold(im_grayscale, <span class="dv">0</span>, <span class="dv">255</span>, cv2.THRESH_OTSU) <span class="co">#auto thresholding</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    im_binary <span class="op">=</span> Image.fromarray(im_binary_array)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> im_binary      </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s try binarising the difference using the OTSU thresholding method, which will automatically pick a pixel threshold value.</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>GSandBinarize(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<p><img src="index_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The backgound is not too shabby; it’s almost fully blacked out, but the horse body has a lot of black areas. Now let’s try binarising the difference using the binary thresholding method, which allows us to pick a pixel threshold value.</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<p><img src="index_files/figure-html/cell-16-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Now, in this case, more of the horse’s body is white, making the mask better at capturing the subject; but lowering the pixel threshold manually has given rise to many white patches in the background as well. Let’s try solving this.</p>
<p><em>SideNote: For the exact same model, random seed, and parameters, the mask seems to come out slightly better on a A4000 GPU, as compared to a P5000. All outputs shown here are from running inference on a Paperspace A4000 instance.</em></p>
<p>Now that we have the whole mask generation process ready, let’s play a bit with the <code>num_inference_steps</code> parameter to see if we can get a better mask!! Let’s start with <code>num_inference_steps=6</code>.</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase (lower number of inf steps)</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">6</span>).images</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>image[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e9071831310549b496214f27db762533","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="32">
<p><img src="index_files/figure-html/cell-17-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase (lower number of inf steps)</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>image_q <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">6</span>).images</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>image_q[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"904d4412615b46518ad46f4863a089d1","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="33">
<p><img src="index_files/figure-html/cell-18-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs <span class="op">=</span> norm_diff_abs(image_q[<span class="dv">0</span>], image[<span class="dv">0</span>])</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<p><img src="index_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<p><img src="index_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>As we can see, this leads to a much clearer outline of the horse than before, but the image is littered with too many white patches. I suspect that that can be solved by averaging out multiple outputs (see next section), and so <code>num_inference_steps=6</code> can be considered a good candidate for our final run. Now let’s try <code>num_inference_steps=8</code>:</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">8</span>).images</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>image[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"57a89009c58f4041b6662374e23ac4c4","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="36">
<p><img src="index_files/figure-html/cell-21-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>image_q <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">8</span>).images</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>image_q[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"00e85c64f96d436697cb031879b7254d","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="37">
<p><img src="index_files/figure-html/cell-22-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs <span class="op">=</span> norm_diff_abs(image_q[<span class="dv">0</span>], image[<span class="dv">0</span>])</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<p><img src="index_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<p><img src="index_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Well why not use a higher <code>num_inference_steps</code> to allow for more denoising? Fair point; but what often seems to happen is that, weird artifacts are generated at a high value of <code>num_inference_steps</code> :</p>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>image_q <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">30</span>).images</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>image_q[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0ece78f5aa7b431a917622cb47c666fe","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="74">
<p><img src="index_files/figure-html/cell-25-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>A super clean-looking image, but now the zebra has two heads!</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span> <span class="co">#Denoising image with respect to the identifying phrase</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">1</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">30</span>).images</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>image[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"54c5e28ee6a547de9dda0a1f3c1971d9","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="43">
<p><img src="index_files/figure-html/cell-26-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs <span class="op">=</span> norm_diff_abs(image_q[<span class="dv">0</span>], image[<span class="dv">0</span>])</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<p><img src="index_files/figure-html/cell-27-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<p><img src="index_files/figure-html/cell-28-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>It’s still just as noisy as the mask outputs we got using a much lower value of <code>num_inference_steps</code>, and now, the outline has become less accurate because of all the extra heads in the denoised images! Let’s stick to a lower value for <code>num_inference_steps</code> and come up with a different way to get a clearer mask.</p>
</section>
<section id="multi-output---based-mask-generation" class="level2">
<h2 class="anchored" data-anchor-id="multi-output---based-mask-generation">Multi output - based mask generation</h2>
<p>Since we are denoising a image after adding Gaussian noise to it, we could try to generate multiple denoised images for each prompt, and average them out before taking the normalised difference of query versus reference. The idea is that averaging multiple images denoised based on the same prompt will help cancel out the random noise, and also create a clearer image of the model’s idea of a “zebra” or “horse”. <br> <em>Note: Judging from the DiffEdit paper’s schematic of the process, this is likely the method adopted by its authors.</em></p>
<div class="cell" data-execution_count="108">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>pipe.enable_attention_slicing()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="164">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dark-horse-riverv2-rsz</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>init_image <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">"dark-horse-riverV2-rsz.jpg"</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>pixels <span class="op">=</span> init_image.load()</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>init_image.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-30-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><em>The image has been cropped and resized to avoid a CUDA OOM error when running for a high value of</em> <code>num_images_per_prompt</code>.</p>
<p>First, we try to implement this with 10 images per prompt, denoised for <code>num_inference_steps=6</code>.</p>
<div class="cell" data-execution_count="109">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>images_r6 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">6</span>).images</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_r6, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"779c267c6949488ba54b5c47541485ad","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="109">
<p><img src="index_files/figure-html/cell-31-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="110">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>images_q6 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">6</span>).images</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_q6, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"36b74dc74e324a649eefebd4018605d2","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="110">
<p><img src="index_files/figure-html/cell-32-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s build out the function for passing in the list of images; and getting the averaged out image! How? Stack the list of images generated and create an average of them, along the “number of images” axis. <br> <em>Note - This function, like all others, was built by experimenting with each line of its components and then putting them together - a nifty trick, courtesy of Jeremy!</em></p>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_averageIm(ImList):</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    imtensors <span class="op">=</span> []</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> Im <span class="kw">in</span> ImList:</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>        imtensor <span class="op">=</span> convert_to_tensor(Im)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>        imtensors.append(imtensor)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    init_tensor <span class="op">=</span> torch.zeros(imtensors[<span class="dv">0</span>].shape)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tensor <span class="kw">in</span> imtensors:</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        init_tensor <span class="op">+=</span> tensor</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    total_tensor <span class="op">=</span> init_tensor</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    av_imtensor <span class="op">=</span> (total_tensor)<span class="op">/</span><span class="dv">10</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    av_image <span class="op">=</span> convert_to_image(av_imtensor)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> av_image</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="111">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>av_image_r6 <span class="op">=</span> get_averageIm(images_r6)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>av_image_r6</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="111">
<p><img src="index_files/figure-html/cell-34-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="112">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>av_image_q6 <span class="op">=</span> get_averageIm(images_q6)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>av_image_q6</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="112">
<p><img src="index_files/figure-html/cell-35-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Like it or not, this is what peak “horse” and “zebra” performance look like! (reference - <a href="https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb">Chapter 4 of fastbook</a>).</p>
<div class="cell" data-execution_count="113">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs6 <span class="op">=</span> norm_diff_abs(av_image_r6, av_image_q6)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>GSandBinarise(im_diff_normabs6, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="113">
<p><img src="index_files/figure-html/cell-36-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="115">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>mask_6step <span class="op">=</span> GSandBinarise(im_diff_normabs6, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>mask_6step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="115">
<p><img src="index_files/figure-html/cell-37-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Repeat the same for other <code>num_inference_steps=8</code> values to get a better mask. Also remember to play around with manual threshold value, to get ideal mask. <br></p>
<p>The mask output looked decent for <code>num_inference_steps=8</code> in the single-output section, so let’s try that here:</p>
<div class="cell" data-execution_count="116">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>images_r8 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">8</span>).images</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_r8, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"76dac886e1c74328b39fd6d77c39d97e","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="116">
<p><img src="index_files/figure-html/cell-38-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="117">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>images_q8 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">8</span>).images</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_q8, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e17346df6572475698f9afb835e00fa1","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="117">
<p><img src="index_files/figure-html/cell-39-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="118">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>av_image_r8 <span class="op">=</span> get_averageIm(images_r8)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>av_image_r8</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="118">
<p><img src="index_files/figure-html/cell-40-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="119">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>av_image_q8 <span class="op">=</span> get_averageIm(images_q8)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>av_image_q8</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="119">
<p><img src="index_files/figure-html/cell-41-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="120">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs8 <span class="op">=</span> norm_diff_abs(av_image_r8, av_image_q8)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>mask_otsu_8step <span class="op">=</span> GSandBinarise(im_diff_normabs8, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>mask_otsu_8step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="120">
<p><img src="index_files/figure-html/cell-42-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="123">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>mask_8step <span class="op">=</span> GSandBinarise(im_diff_normabs8, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>mask_8step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="123">
<p><img src="index_files/figure-html/cell-43-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>That looks better than the previous mask. We’ve considerably reduced the white patches in the background. Let’s try <code>num_inference_steps=7</code>.</p>
<div class="cell" data-execution_count="125">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co">#inf steps 7</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>images_r7 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">7</span>).images</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>image_grid(images_r7, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"397fc3f58a744b90b120d0652a623be8","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="125">
<p><img src="index_files/figure-html/cell-44-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="126">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>images_q7 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">7</span>).images</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_q7, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9c8868ca16f9462fa6dd6f11e7c35a96","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="126">
<p><img src="index_files/figure-html/cell-45-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="129">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>av_image_r7 <span class="op">=</span> get_averageIm(images_r7)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>av_image_r7</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="129">
<p><img src="index_files/figure-html/cell-46-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="130">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>av_image_q7 <span class="op">=</span> get_averageIm(images_q7)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>av_image_q7</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="130">
<p><img src="index_files/figure-html/cell-47-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="131">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs7 <span class="op">=</span> norm_diff_abs(av_image_r7, av_image_q7)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>mask_otsu_7step <span class="op">=</span> GSandBinarise(im_diff_normabs7, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>mask_otsu_7step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="131">
<p><img src="index_files/figure-html/cell-48-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="132">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>mask_7step <span class="op">=</span> GSandBinarise(im_diff_normabs7, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>mask_7step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="132">
<p><img src="index_files/figure-html/cell-49-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This mask (<code>num_inference_steps=7</code>) has a slightly cleaner outline than the one for <code>num_inference_steps=8</code>. Let’s try going a bit higher; <code>num_inference_steps=10</code></p>
<div class="cell" data-execution_count="133">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>images_r10 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">10</span>).images</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_r10, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"765870237c4a49228b7a3b3a1699385e","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="133">
<p><img src="index_files/figure-html/cell-50-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="134">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>images_q10 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">10</span>).images</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_q10, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"87e5dfaae6ff4ccbbbc77b74294019f4","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="134">
<p><img src="index_files/figure-html/cell-51-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>As one can see, there are a few weird images, and all the images are quite deformed with respect to the original <code>init_image</code>. Let’s look at the generated mask.</p>
<div class="cell" data-execution_count="135">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>av_image_r10 <span class="op">=</span> get_averageIm(images_r10)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>av_image_r10</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>av_image_q10 <span class="op">=</span> get_averageIm(images_q10)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>av_image_q10</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="135">
<p><img src="index_files/figure-html/cell-52-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="137">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs10 <span class="op">=</span> norm_diff_abs(av_image_r10, av_image_q10)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>mask_otsu_10step <span class="op">=</span> GSandBinarise(im_diff_normabs10, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>mask_otsu_10step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="137">
<p><img src="index_files/figure-html/cell-53-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="138">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>mask_10step <span class="op">=</span> GSandBinarise(im_diff_normabs10, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>mask_10step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="138">
<p><img src="index_files/figure-html/cell-54-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We start to see a more patchy outline, because the denoised images have deviated more and more from the original outline. Let’s go down one step and try <code>num_inference_steps=9</code>.</p>
<div class="cell" data-execution_count="142">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse drinking water"</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>images_r9 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">9</span>).images</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_r9, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"17ead7d5880b4e4f81d465d1f067cea2","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="142">
<p><img src="index_files/figure-html/cell-55-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="139">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1000</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>prompt_q <span class="op">=</span> <span class="st">"a zebra drinking water"</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>images_q9 <span class="op">=</span> pipe(prompt<span class="op">=</span>prompt_q, num_images_per_prompt<span class="op">=</span><span class="dv">10</span>, init_image<span class="op">=</span>init_image, strength<span class="op">=</span><span class="fl">0.8</span>, num_inference_steps<span class="op">=</span><span class="dv">9</span>).images</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>image_grid(images_q9, rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"38542981fc124410840d767beb5bf778","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="139">
<p><img src="index_files/figure-html/cell-56-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The images are still weird and vary considerably in the position of the subject, so we can expect the mask to be “noisy” as well.</p>
<div class="cell" data-execution_count="143">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>av_image_r9 <span class="op">=</span> get_averageIm(images_r9)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>av_image_r9</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="143">
<p><img src="index_files/figure-html/cell-57-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="144">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>av_image_q9 <span class="op">=</span> get_averageIm(images_q9)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>av_image_q9</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="144">
<p><img src="index_files/figure-html/cell-58-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="145">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>im_diff_normabs9 <span class="op">=</span> norm_diff_abs(av_image_r9, av_image_q9)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>mask_otsu_9step <span class="op">=</span> GSandBinarise(im_diff_normabs9, thresh_method<span class="op">=</span><span class="st">'OTSU'</span>)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>mask_otsu_9step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="145">
<p><img src="index_files/figure-html/cell-59-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="146">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>mask_9step <span class="op">=</span> GSandBinarise(im_diff_normabs9, thresh_method<span class="op">=</span><span class="st">'manual'</span>, thresh<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>mask_9step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="146">
<p><img src="index_files/figure-html/cell-60-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Though better than the previous run, this still has a rather unclear outline. From the above analyses, we can pick <code>num_inference_steps=6, 7, 8</code> as yielding usable pixel masks.</p>
<div class="cell" data-execution_count="168">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>mask_list <span class="op">=</span> [mask_6step, mask_7step, mask_8step]</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>image_grid(mask_list, rows<span class="op">=</span><span class="dv">1</span>, cols<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="168">
<p><img src="index_files/figure-html/cell-61-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>If we could see how these masks look on the original image, I suppose they’d be more convincing. I’m currently facing a minor bug in overlaying a red mask onto the original image; and this notebook will be updated with the same once that is resolved.</p>
<p>Though a far cry from the clean mask illustrated in the DiffEdit paper, I suppose this isn’t a bad start. As I experiment with other techniques (like using other noise schedulers) to generate a fully clean mask, I’ll be adding relevant updates on this post. <br></p>
<p>Hopefully you got something from your time here! If you read this and have any suggestions/comments on how to improve my code (or words!), please reach out to me <a href="https://twitter.com/jamesemilian2"><span class="citation" data-cites="Twitter">@Twitter</span></a>. <br></p>
<p>Cheers!! 😄</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>