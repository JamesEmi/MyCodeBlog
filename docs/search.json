[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBlood Mask EDA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheory of Mind for Robotics - A Discussion of SOTA, Neural Bases, and Challenges\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLessons from Deploying ML Models on NVIDIA Orin & Jetson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttention - contrasting the neural perspective with the mathematical\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText prompt-based Image Masking using Stable Diffusion: A series of experiments\n\n\n\n\n\n\nCode\n\n\nAI\n\n\nStableDiffusion\n\n\nfast.ai\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nJames Emilian\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello :)",
    "section": "",
    "text": "I am a researcher and engineer with expertise in Machine Learning, Robotics, and AI for Healthcare. My work spans across academia and industry, with a focus on deploying intelligent systems for real-world impact.\nI am currently a core ML team member at the DARPA Triage Challenge, developing multimodal deep learning models for battlefield trauma detection.\nI‚Äôve worked on diverse research areas, including: - Machine Unlearning for privacy-preserving AI - Imitation Learning for long-horizon robotics - AI-powered ultrasound diagnostics for medical applications\nüìå Check out my Projects and Research.\n‚úçÔ∏è For my thoughts on AI & research, visit the Blog.\n\n\n\n\n\nJames Emilian"
  },
  {
    "objectID": "darpa.html",
    "href": "darpa.html",
    "title": "DARPA Triage Challenge",
    "section": "",
    "text": "ML Algorithms Core Member, CMU Robotics Institute\n\n\nWe developed multimodal deep learning models for stand-off vital sensing and hemorrhage detection in battlefield scenarios.\n\n\n\n\nBuilt CNN + Transformer hybrid models for heart and respiratory rate estimation.\nDeployed on NVIDIA Jetson Orin for real-time inference.\nField-tested on diverse robotic platforms for DARPA Challenge Year 1.\nManuscript awarded Best Thesis Award.\n\n\n\n\n\nDeployment on Jetson Orin\nRobotic Platform Field Test\n\n\nüîô Back to Projects"
  },
  {
    "objectID": "darpa.html#overview",
    "href": "darpa.html#overview",
    "title": "DARPA Triage Challenge",
    "section": "",
    "text": "We developed multimodal deep learning models for stand-off vital sensing and hemorrhage detection in battlefield scenarios."
  },
  {
    "objectID": "darpa.html#research-contributions",
    "href": "darpa.html#research-contributions",
    "title": "DARPA Triage Challenge",
    "section": "",
    "text": "Built CNN + Transformer hybrid models for heart and respiratory rate estimation.\nDeployed on NVIDIA Jetson Orin for real-time inference.\nField-tested on diverse robotic platforms for DARPA Challenge Year 1.\nManuscript awarded Best Thesis Award."
  },
  {
    "objectID": "darpa.html#video-demonstrations",
    "href": "darpa.html#video-demonstrations",
    "title": "DARPA Triage Challenge",
    "section": "",
    "text": "Deployment on Jetson Orin\nRobotic Platform Field Test\n\n\nüîô Back to Projects"
  },
  {
    "objectID": "posts/attention_neural/index.html",
    "href": "posts/attention_neural/index.html",
    "title": "Attention - contrasting the neural perspective with the mathematical",
    "section": "",
    "text": "The attention mechanism in the brain directly analogous to the attention mechanism in models. Can I draw parallels??\n(really leaning into this angle is a fun play to make/)\nReference links:\nThe concept of change blindness: \nCould not see the wind shield color change; and once it was mentioned - yes it was. Then - after it was pointed out - we could notice it even with the peripheral vision. Can I run a model on this image and illustrate how this works?\nFinding Waldo - a case of feature attention - it‚Äôs hard because it‚Äôs a case of conjunction search - wherein you have to search for a fusion of multiple features alongside each other. It‚Äôs a hard problem.\nQues - do models do this?\nFeature search is thought to be a parallel process; however conjunction search is thought to be a serial process. This make the latter harder.\nIdea: the chicken experiment - invalid and valid cues. (Can this be a new form of data augemntation mechanism for training attention mechanisms?)\nRead: - Even chickens can attend"
  },
  {
    "objectID": "posts/ToM_11-851/index.html",
    "href": "posts/ToM_11-851/index.html",
    "title": "Theory of Mind for Robotics - A Discussion of SOTA, Neural Bases, and Challenges",
    "section": "",
    "text": "Here I am, procrastinating writing my thesis because I am fascinated with this new topic aka ToM - thankfully, I was introduced to it by one who understands it from both a NLP & cognitive standpoint as well as from a robotics standpoint - Prof.¬†Yonatan Bisk! I‚Äôll note down some points from my convo with him; read the papers suggested, and try to think of why this is hard and what we can do to solve it.\nMy question to him: ‚ÄúAllow me to be annoying for a minute here and reopen the Theory of Mind can of worms once again! I should probably do the readings first; but while I get started on those I do want to ask again about why you said that ToM remains a highly challenging topic - even annoying, because everything is reduced to a low-D problem and solved. You said solving ToM would essentially mean ‚Äòsolving complete AI‚Äô. Maybe I missed the point in the class, but what is hard about solving the ‚Äòrecursive state update‚Äô problem that we discussed as being functionally similar to Theory of Mind?‚Äù\nHis answer:\n‚ÄúIf the full form of theory of mind is an accurate model of a latent representation that you cannot ever observe, then you have to infer the model from the behaviors and contexts. So you need to be able to run a simulation of what you would do in this situation to lead you to say/do the same things, but you don‚Äôt know the whole context so you might also need to hallucinate stories that would likely give the context necessary to explain the behavior but you would need to be able to do that well because you‚Äôll probably never get supervision for that process and if you get it wrong then you will ascribe the wrong intention to people which typically means the social dynamic goes south ‚Äì or as you pointed out in ASD this can lead to frustration and depression because you can‚Äôt predict why people are doing what they are doing. But to run a simulation like this means you already need to have your own model of the world (physical and social) that you use for yourself and maybe are reusing to simulate others. So we could simplify all of this and say it‚Äôs just a knowledge problem ‚Äì the base form ‚Äì‚ÄùA knows X, B saw Y, ‚Ä¶‚Äù but the social world you are actually trying to model requires you understand how to build a model of the world ‚Äì affordances, actions, etc, that you can run simulations, can evaluate possible pasts and futures, can align those to your observations, can update your understanding for that person and perhaps all people, etc.. If I could do that, what couldn‚Äôt I do?‚Äù\nWrite about my theory - that I think that reading can play a huge role in developing a semblance of ToM in ASD kids (or in people, for that matter). I hold this as a strong opinion, because I read a lot of fiction and other things as a kid; and felt like I had a fairly articulated world model. Although - I was socially awkward for a long time; that likely points to the fact that you need some sort of environmental grounding - ‚Äúokay, I can say X in context Y but not in context Z‚Äù, ‚ÄúIf A says G when we are in group D, it means this; here is what reactions of B, C and D to what A said reveal to me about A‚Äôs standing in the group and of B, C and D‚Äôs leanings‚Äù etc. The fact that I had to look for those things were in some sense inspired by reading books and seeing characters do that; but there was surely a phase of me trying to be in this ‚Äògrounding‚Äô phase, and go through a lot of ‚Äòpain‚Äô for grounding failures - and once I had learned the dynamic, I could now be a part of this group. The reason I started saying this is: - I think there can be supervision to world model building in a robot - just use all the quality good fiction that the human race has generated as training data. However that does bring into question the model being used to learn the patterns in the training data. The human brain is capable of ‚Äòtrue reasoning‚Äô, and we already established in class that GPT cannot do that - its few failure cases simply show that it‚Äôs not doing true reasoning. Inherent flaw, will some new magic neuro-inspired model be able to fix this?\n\nGPT on Prof.Bisk‚Äôs answer\nBreaking Down Prof.¬†Bisk‚Äôs Answer:\n\nUnderstanding Theory of Mind (ToM): Latent Representations: At its core, ToM involves understanding that others have internal mental states (beliefs, desires, intentions) that are not directly observable. Inference from Behavior and Context: Since we cannot access these internal states directly, we must infer them by observing behaviors and understanding the context in which they occur.\nSimulating Others‚Äô Perspectives: Running Simulations: To predict or explain someone‚Äôs behavior, we simulate what we might do in their situation to produce similar actions or words. Incomplete Information: We often lack complete context about others‚Äô situations, so we need to hypothesize or ‚Äúhallucinate‚Äù plausible stories that could explain their behavior. Accuracy is Critical: If our inferred models are incorrect, we may misattribute intentions, leading to misunderstandings or social friction. Connection to ASD: Individuals with Autism Spectrum Disorder may struggle with ToM, leading to difficulties in predicting behaviors and social interactions, which can cause frustration and emotional distress.\nRequirements for Effective Simulation: Personal Model of the World: To simulate others, we need a robust understanding of how the world works, including physical laws and social norms. Reusing Our Models: We often use our own mental frameworks and experiences as a basis for simulating others‚Äô perspectives.\nSimplifying vs.¬†Complexity: Knowledge Problem Simplification: One might reduce ToM to tracking who knows what (e.g., ‚ÄúA knows X,‚Äù ‚ÄúB saw Y‚Äù), but this oversimplifies the complexity of social cognition. Complex Social Modeling: Accurately modeling the social world involves understanding: Affordances: What actions are possible in a given context. Actions and Consequences: How actions lead to outcomes. Simulations of Pasts and Futures: Imagining different scenarios to explain behaviors. Alignment with Observations: Ensuring our simulations match what we observe. Updating Understanding: Refining our models based on new information about individuals and groups.\nThe Ultimate Challenge in AI: General Intelligence: If an AI could perform all these tasks effectively, it would essentially possess a level of understanding and adaptability akin to human intelligence. Open-Ended Abilities: Mastering ToM in AI implies solving numerous complex problems, leaving little that the AI couldn‚Äôt do in terms of social cognition and interaction.\n\n\n\nPseudoCode and Thoughts\n\nclass WorldModel:\n    def __init__(self):\n        self.physical_laws = load_physical_laws()\n        self.social_norms = load_social_norms()\n        self.knowledge_base = initialize_knowledge_base()\n\n    def update(self, observations):\n        # Update the world model based on new observations\n        self.knowledge_base.update(observations)\n\nclass AgentModel:\n    def __init__(self, world_model):\n        self.world_model = world_model\n        self.beliefs = {}\n        self.desires = {}\n        self.intentions = {}\n\n    def infer_mental_states(self, observed_behavior, context):\n        # Infer the agent's mental states from behavior and context\n        possible_states = generate_possible_states(context)\n        best_match = evaluate_states(possible_states, observed_behavior)\n        self.beliefs, self.desires, self.intentions = best_match\n\n    def simulate_behavior(self):\n        # Simulate behavior based on inferred mental states\n        simulated_behavior = run_simulation(self.beliefs, self.desires, self.intentions)\n        return simulated_behavior\n\ndef generate_possible_states(context):\n    # Generate hypotheses about the agent's mental states\n    hypotheses = []\n    for possible_belief in context.possible_beliefs():\n        for possible_desire in context.possible_desires():\n            for possible_intention in context.possible_intentions():\n                hypotheses.append((possible_belief, possible_desire, possible_intention))\n    return hypotheses\n\ndef evaluate_states(possible_states, observed_behavior):\n    # Evaluate which hypothesized states best explain the observed behavior\n    best_match = None\n    highest_score = float('-inf')\n    for state in possible_states:\n        simulated_behavior = run_simulation(*state)\n        score = compare_behavior(simulated_behavior, observed_behavior)\n        if score &gt; highest_score:\n            highest_score = score\n            best_match = state\n    return best_match\n\ndef run_simulation(beliefs, desires, intentions):\n    # Simulate behavior based on mental states\n    # This function would involve complex reasoning and prediction\n    simulated_behavior = complex_reasoning_engine(beliefs, desires, intentions)\n    return simulated_behavior\n\ndef compare_behavior(simulated_behavior, observed_behavior):\n    # Compare simulated behavior with observed behavior to score the hypothesis\n    score = similarity_metric(simulated_behavior, observed_behavior)\n    return score\n\ndef main():\n    world_model = WorldModel()\n    agent_model = AgentModel(world_model)\n\n    while True:\n        observations = get_new_observations()\n        world_model.update(observations['context'])\n\n        observed_behavior = observations['behavior']\n        context = world_model.knowledge_base\n\n        agent_model.infer_mental_states(observed_behavior, context)\n        predicted_behavior = agent_model.simulate_behavior()\n\n        # Use predicted behavior for further reasoning or interaction\n        take_action_based_on_prediction(predicted_behavior)\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "posts/trial_2024/index.html",
    "href": "posts/trial_2024/index.html",
    "title": "Blood Mask EDA",
    "section": "",
    "text": "Take a look at the data I have now - how a blood mask png generated on Pixelmator looks:\n\nfrom PIL import Image\nimport numpy as np\n\nmask = Image.open('/notebooks/triage/hemofpn/data_v2/train/masks/02.png')\n\nmask_arr = np.array(mask)\nmask_arr.shape\n\n(723, 758, 4)\n\n\n\nnp.unique(mask_arr)\n\narray([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n       247, 248, 249, 250, 251, 252, 253, 254, 255], dtype=uint8)\n\n\nWhat does the image itself look like?\n\nfrom PIL import Image\nimport numpy as np\n\nimage = Image.open('/notebooks/triage/hemofpn/data_v2/train/images/02.png')\n\nimage_arr = np.array(image)\nimage_arr.shape\n\n(723, 758, 4)\n\n\n\ndef pad_im(image, target_shape=(768, 768)):\n        if image.ndim == 3 and (image.shape[2] == 3 or image.shape[2] == 1):  # If image is in HWC format\n            h, w, c = image.shape\n            image = np.transpose(image, (2, 0, 1))  # Convert to CHW for processing\n        elif image.ndim == 3 and (image.shape[0] == 3 or image.shape[0] == 1):  # If image is in CHW format\n            c, h, w = image.shape\n            # image = np.transpose(image, (1, 2, 0))  # Convert to HWC for processing\n        elif image.ndim == 2:\n            h, w = image.shape\n        else:\n            raise ValueError(\"Unexpected image format, image is of shape:\", image.shape)\n        \n        scale = min(target_shape[0] / h, target_shape[1] / w)\n        new_h, new_w = int(h * scale), int(w * scale)\n\n        # Ensure dimensions are divisible by 32\n        new_h = ((new_h + 31) // 32) * 32\n        new_w = ((new_w + 31) // 32) * 32\n\n        resized_image = np.array([cv2.resize(img, (new_w, new_h)) for img in image])\n\n        pad_h = max(target_shape[0] - new_h, 0)\n        pad_w = max(target_shape[1] - new_w, 0)\n        pad_top = pad_h // 2\n        pad_bottom = pad_h - pad_top\n        pad_left = pad_w // 2\n        pad_right = pad_w - pad_left\n\n        padded_im = np.pad(resized_image, ((0, 0), (pad_top, pad_bottom), (pad_left, pad_right)), mode='constant')\n\n        return padded_im\n\n\n!pip install opencv-python\n\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (4.6.0.66)\nRequirement already satisfied: numpy&gt;=1.17.3 in /usr/local/lib/python3.9/dist-packages (from opencv-python) (1.23.4)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nimport matplotlib.pyplot as plt\n\n\nimage.shape\n\n(723, 758, 3)\n\n\n\nimage = np.transpose(image, (2, 0, 1))\nimage.shape\n\n(3, 723, 758)\n\n\n\nimage = pad_im(image)\nimage.shape\n\n(3, 768, 768)\n\n\nOk, after padding the image has this shape.\nHow does a blood mask png from F8 look?\n\nmask_f8 = Image.open('/notebooks/triage/hemofpn/data_f8/train/masks/adt2_image_1121.png')\n\nmask_f8 = np.array(mask_f8)\nmask_f8.shape\n\n(1440, 1920, 4)\n\n\n\nnp.unique(mask_f8)\n\narray([  0, 255], dtype=uint8)\n\n\n\nimport cv2\n\nimage = cv2.imread('/notebooks/triage/hemofpn/data_f8/train/masks/adt2_image_1121.png')\n\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.imshow(image)\n\n\n\n\n\n\n\n\n\nimage.shape\n\n(1440, 1920, 3)\n\n\nLooks ok - the only difference is that with F8 masks the constituent values are only 0 and 255."
  },
  {
    "objectID": "posts/DiffEditMask/index.html",
    "href": "posts/DiffEditMask/index.html",
    "title": "Text prompt-based Image Masking using Stable Diffusion: A series of experiments",
    "section": "",
    "text": "Inspired by the amazing paper ‚ÄúDiffEdit: Diffusion-based semantic image editing with mask guidance‚Äù which Jeremy discussed in last week‚Äôs class of Practical Deep Learning for Coders Pt2, this is an attempt (v1) at implementing step one of the paper‚Äôs semantic image editing method.\nSee below an image from the paper, illustrating this novel method of editing images simply using a text query:\n\nSeeing the automatic mask generation was fascinating for me, and it was evident from interactions during the class that this was indeed a novel, interesting way to generate pixel masks. Stable Diffusion, the poster-child for modern AI, and the basis for cool tech like Dream Studio and Midjourney, could do something more pragmatic too! The mask generation strategy is simple - it involves using stable diffusion to create a pixel mask for an input image, simply using two text prompts. Instead of going off on an verbose explanation I‚Äôll allow a screenshot from the DiffEdit paper to do the job. \n\nIf you wish to delve deeper into Stable Diffusion I highly recommend going through this amazing repo from fast.ai - diffusion nbs on GitHub.\nTo quote the paper, for the mask creation - ‚ÄúWe add noise to the input image, and denoise it: once conditioned on the query text, and once conditioned on a reference text (or unconditionally). We derive a mask based on the difference in the denoising results‚Äù. The above screenshot is part of a detailed schematic - where the input is the image x0 and query text Q = 'Zebra' and reference text R = 'Horse'. The StableDiffusionImg2ImgPipeline pipeline from HuggingFace takes care of adding the Gaussian noise to an input image, and denoising the resulting image by running inference using a pretrained model. We can play around with the inference input parameters like input image, number of inference steps, strength of prompt guidance, etc. to influence the denoised image output.  Note: This post documents multiple experiments, with varying levels of quality of final output. The intent is to capture the thought process and important trial and error and resulting insights that went into trying to generate a good pixel mask using SD.\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nLoad the pretrained StableDiffusionImg2ImgPipeline onto pipe:\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\",revision=\"fp16\",torch_dtype=torch.float16, safety_checker = None).to(\"cuda\")\n\nLet‚Äôs load and take a look at the input image, x0 :\n\ninit_image = Image.open(\"dark-horse-riverV2.jpeg\")\npixels = init_image.load()\ninit_image.show()"
  },
  {
    "objectID": "posts/DiffEditMask/index.html#introduction-setup",
    "href": "posts/DiffEditMask/index.html#introduction-setup",
    "title": "Text prompt-based Image Masking using Stable Diffusion: A series of experiments",
    "section": "",
    "text": "Inspired by the amazing paper ‚ÄúDiffEdit: Diffusion-based semantic image editing with mask guidance‚Äù which Jeremy discussed in last week‚Äôs class of Practical Deep Learning for Coders Pt2, this is an attempt (v1) at implementing step one of the paper‚Äôs semantic image editing method.\nSee below an image from the paper, illustrating this novel method of editing images simply using a text query:\n\nSeeing the automatic mask generation was fascinating for me, and it was evident from interactions during the class that this was indeed a novel, interesting way to generate pixel masks. Stable Diffusion, the poster-child for modern AI, and the basis for cool tech like Dream Studio and Midjourney, could do something more pragmatic too! The mask generation strategy is simple - it involves using stable diffusion to create a pixel mask for an input image, simply using two text prompts. Instead of going off on an verbose explanation I‚Äôll allow a screenshot from the DiffEdit paper to do the job. \n\nIf you wish to delve deeper into Stable Diffusion I highly recommend going through this amazing repo from fast.ai - diffusion nbs on GitHub.\nTo quote the paper, for the mask creation - ‚ÄúWe add noise to the input image, and denoise it: once conditioned on the query text, and once conditioned on a reference text (or unconditionally). We derive a mask based on the difference in the denoising results‚Äù. The above screenshot is part of a detailed schematic - where the input is the image x0 and query text Q = 'Zebra' and reference text R = 'Horse'. The StableDiffusionImg2ImgPipeline pipeline from HuggingFace takes care of adding the Gaussian noise to an input image, and denoising the resulting image by running inference using a pretrained model. We can play around with the inference input parameters like input image, number of inference steps, strength of prompt guidance, etc. to influence the denoised image output.  Note: This post documents multiple experiments, with varying levels of quality of final output. The intent is to capture the thought process and important trial and error and resulting insights that went into trying to generate a good pixel mask using SD.\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nLoad the pretrained StableDiffusionImg2ImgPipeline onto pipe:\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\",revision=\"fp16\",torch_dtype=torch.float16, safety_checker = None).to(\"cuda\")\n\nLet‚Äôs load and take a look at the input image, x0 :\n\ninit_image = Image.open(\"dark-horse-riverV2.jpeg\")\npixels = init_image.load()\ninit_image.show()"
  },
  {
    "objectID": "posts/DiffEditMask/index.html#single-output---based-mask-generation",
    "href": "posts/DiffEditMask/index.html#single-output---based-mask-generation",
    "title": "Text prompt-based Image Masking using Stable Diffusion: A series of experiments",
    "section": "Single output - based mask generation",
    "text": "Single output - based mask generation\nIn this case, we‚Äôll keep num_images_per_prompt=1 in the image-image stable diffusion pipeline, and see how good a mask we can generate using that. We‚Äôll play around a bit with num_inference_steps to create the best possible mask.\npipe takes care of adding noise to the input image init_image, and denoising it for num_inference_steps=8, with respect to the reference text - \"a horse\".\n\ntorch.manual_seed(1000)\nprompt = \"a horse\" #Denoising image with respect to the identifying phrase \nimage = pipe(prompt=prompt, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=10).images\nimage[0]\n\n\n\n\n\n\n\n\n\n\n\nThat‚Äôs a scary looking horse, one must admit! For some reason I‚Äôve seen from my experiments (not recorded here) that the model often grows extra appendages from the tail pixels of horse images. We can try reducing num_inference_steps to prevent that, but the downside is that the output image will be noisy, since we haven‚Äôt allowed for enough inference steps. For now, we‚Äôll move on and see what happens with the query text denoising.\nWe use pipe to add noise to the input image init_image, and denoise it for num_inference_steps=10, with respect to the query text - \"a zebra\".\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra\" #Denoising image with respect to the identifying phrase\nimage_q = pipe(prompt=prompt_q, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=10).images\nimage_q[0]\n\n\n\n\n\n\n\n\n\n\n\nThat was unexpected! Let‚Äôs reduce num_inference_steps to 8 to prevent the inference from going too far!\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra\" #Denoising image with respect to the identifying phrase\nimage_q = pipe(prompt=prompt_q, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=8).images\nimage_q[0]\n\n\n\n\n\n\n\n\n\n\n\nBetter. Notice how the background is changing considerably based on the prompt. Let‚Äôs try altering the prompt to make sure the backgrounds are relatively similar in the images denoised with respect to both Query Q and Reference R.\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\" #Denoising image with respect to the identifying phrase\nimage = pipe(prompt=prompt, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=10).images\nimage[0]\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\" #Denoising image with respect to the identifying phrase\nimage_q = pipe(prompt=prompt_q, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=10).images\nimage_q[0]\n\n\n\n\n\n\n\n\n\n\n\nNotice how giving more information in the prompt about the action of the subject with the input ‚Äúa zebra drinking water‚Äù has allowed the model to run for 10 inference steps without morphing the zebra too badly!\nNeither of those are particularly pretty images on their own; but let‚Äôs attempt to find the ‚Äúnormalised difference between the denoised images‚Äù, and generate a rough mask. Then we can try to play with num_inference_steps and create a better mask. Once that parameter‚Äôs ideal value is clear we can move to trying to use multiple denoised images.\n\nfrom torchvision import transforms\nconvert_to_tensor = transforms.ToTensor()\nconvert_to_image = transforms.ToPILImage()\n\nSince we‚Äôll be using this operation often, let‚Äôs define a function to compute the normalised absolute difference between two images.\n\ndef norm_diff_abs(a,b):\n    a_tens = convert_to_tensor(a)\n    b_tens = convert_to_tensor(b)\n    ftens = (abs(a_tens-b_tens))/(a_tens+b_tens)\n    fimage = convert_to_image(ftens)\n    return fimage\n\nNote that abs() is an important part of getting a meaningful normalised difference image. Without it the resulting difference image doesn‚Äôt serve our purpose well. Ponder the reason, dear reader ;)\n\nim_diff_normabs = norm_diff_abs(image_q[0], image[0])\n\n\nim_diff_normabs\n\n\n\n\n\n\n\n\nNow that we have the normalised difference between the denoised images (a visual representation of the contrast between reference text and query), let‚Äôs write a function to convert this image to grayscale and binarize this. This function, when called, should yield the final mask M.\n\ndef GSandBinarize(im, **kwargs):\n    im_array = asarray(im)\n    im_grayscale = cv2.cvtColor(im_array, cv2.COLOR_BGR2GRAY)\n    if kwargs[\"thresh_method\"] == 'manual':\n        th, im_binary_array = cv2.threshold(im_grayscale, kwargs[\"thresh\"], 255, cv2.THRESH_BINARY) #manual thresholding\n    else:\n        th, im_binary_array = cv2.threshold(im_grayscale, 0, 255, cv2.THRESH_OTSU) #auto thresholding\n    im_binary = Image.fromarray(im_binary_array)\n    return im_binary      \n\nNow let‚Äôs try binarising the difference using the OTSU thresholding method, which will automatically pick a pixel threshold value.\n\nGSandBinarize(im_diff_normabs, thresh_method='OTSU')\n\n\n\n\n\n\n\n\nThe backgound is not too shabby; it‚Äôs almost fully blacked out, but the horse body has a lot of black areas. Now let‚Äôs try binarising the difference using the binary thresholding method, which allows us to pick a pixel threshold value.\n\nGSandBinarise(im_diff_normabs, thresh_method='manual', thresh=30)\n\n\n\n\n\n\n\n\nNow, in this case, more of the horse‚Äôs body is white, making the mask better at capturing the subject; but lowering the pixel threshold manually has given rise to many white patches in the background as well. Let‚Äôs try solving this.\nSideNote: For the exact same model, random seed, and parameters, the mask seems to come out slightly better on a A4000 GPU, as compared to a P5000. All outputs shown here are from running inference on a Paperspace A4000 instance.\nNow that we have the whole mask generation process ready, let‚Äôs play a bit with the num_inference_steps parameter to see if we can get a better mask!! Let‚Äôs start with num_inference_steps=6.\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\" #Denoising image with respect to the identifying phrase (lower number of inf steps)\nimage = pipe(prompt=prompt, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=6).images\nimage[0]\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\" #Denoising image with respect to the identifying phrase (lower number of inf steps)\nimage_q = pipe(prompt=prompt_q, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=6).images\nimage_q[0]\n\n\n\n\n\n\n\n\n\n\n\n\nim_diff_normabs = norm_diff_abs(image_q[0], image[0])\nGSandBinarise(im_diff_normabs, thresh_method='OTSU')\n\n\n\n\n\n\n\n\n\nGSandBinarise(im_diff_normabs, thresh_method='manual', thresh=30)\n\n\n\n\n\n\n\n\nAs we can see, this leads to a much clearer outline of the horse than before, but the image is littered with too many white patches. I suspect that that can be solved by averaging out multiple outputs (see next section), and so num_inference_steps=6 can be considered a good candidate for our final run. Now let‚Äôs try num_inference_steps=8:\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\" #Denoising image with respect to the identifying phrase\nimage = pipe(prompt=prompt, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=8).images\nimage[0]\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\" #Denoising image with respect to the identifying phrase\nimage_q = pipe(prompt=prompt_q, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=8).images\nimage_q[0]\n\n\n\n\n\n\n\n\n\n\n\n\nim_diff_normabs = norm_diff_abs(image_q[0], image[0])\nGSandBinarise(im_diff_normabs, thresh_method='OTSU')\n\n\n\n\n\n\n\n\n\nGSandBinarise(im_diff_normabs, thresh_method='manual', thresh=30)\n\n\n\n\n\n\n\n\nWell why not use a higher num_inference_steps to allow for more denoising? Fair point; but what often seems to happen is that, weird artifacts are generated at a high value of num_inference_steps :\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\" #Denoising image with respect to the identifying phrase\nimage_q = pipe(prompt=prompt_q, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=30).images\nimage_q[0]\n\n\n\n\n\n\n\n\n\n\n\nA super clean-looking image, but now the zebra has two heads!\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\" #Denoising image with respect to the identifying phrase\nimage = pipe(prompt=prompt, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=30).images\nimage[0]\n\n\n\n\n\n\n\n\n\n\n\n\nim_diff_normabs = norm_diff_abs(image_q[0], image[0])\nGSandBinarise(im_diff_normabs, thresh_method='OTSU')\n\n\n\n\n\n\n\n\n\nGSandBinarise(im_diff_normabs, thresh_method='manual', thresh=30)\n\n\n\n\n\n\n\n\nIt‚Äôs still just as noisy as the mask outputs we got using a much lower value of num_inference_steps, and now, the outline has become less accurate because of all the extra heads in the denoised images! Let‚Äôs stick to a lower value for num_inference_steps and come up with a different way to get a clearer mask."
  },
  {
    "objectID": "posts/DiffEditMask/index.html#multi-output---based-mask-generation",
    "href": "posts/DiffEditMask/index.html#multi-output---based-mask-generation",
    "title": "Text prompt-based Image Masking using Stable Diffusion: A series of experiments",
    "section": "Multi output - based mask generation",
    "text": "Multi output - based mask generation\nSince we are denoising a image after adding Gaussian noise to it, we could try to generate multiple denoised images for each prompt, and average them out before taking the normalised difference of query versus reference. The idea is that averaging multiple images denoised based on the same prompt will help cancel out the random noise, and also create a clearer image of the model‚Äôs idea of a ‚Äúzebra‚Äù or ‚Äúhorse‚Äù.  Note: Judging from the DiffEdit paper‚Äôs schematic of the process, this is likely the method adopted by its authors.\n\npipe.enable_attention_slicing()\n\n\n# dark-horse-riverv2-rsz\ninit_image = Image.open(\"dark-horse-riverV2-rsz.jpg\")\npixels = init_image.load()\ninit_image.show()\n\n\n\n\n\n\n\n\nThe image has been cropped and resized to avoid a CUDA OOM error when running for a high value of num_images_per_prompt.\nFirst, we try to implement this with 10 images per prompt, denoised for num_inference_steps=6.\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\"\nimages_r6 = pipe(prompt=prompt, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=6).images\nimage_grid(images_r6, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\"\nimages_q6 = pipe(prompt=prompt_q, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=6).images\nimage_grid(images_q6, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs build out the function for passing in the list of images; and getting the averaged out image! How? Stack the list of images generated and create an average of them, along the ‚Äúnumber of images‚Äù axis.  Note - This function, like all others, was built by experimenting with each line of its components and then putting them together - a nifty trick, courtesy of Jeremy!\n\ndef get_averageIm(ImList):\n    imtensors = []\n    for Im in ImList:\n        imtensor = convert_to_tensor(Im)\n        imtensors.append(imtensor)\n        \n    init_tensor = torch.zeros(imtensors[0].shape)\n    for tensor in imtensors:\n        init_tensor += tensor\n        \n    total_tensor = init_tensor\n    av_imtensor = (total_tensor)/10\n    av_image = convert_to_image(av_imtensor)\n    \n    return av_image\n\n\nav_image_r6 = get_averageIm(images_r6)\nav_image_r6\n\n\n\n\n\n\n\n\n\nav_image_q6 = get_averageIm(images_q6)\nav_image_q6\n\n\n\n\n\n\n\n\nLike it or not, this is what peak ‚Äúhorse‚Äù and ‚Äúzebra‚Äù performance look like! (reference - Chapter 4 of fastbook).\n\nim_diff_normabs6 = norm_diff_abs(av_image_r6, av_image_q6)\nGSandBinarise(im_diff_normabs6, thresh_method='OTSU')\n\n\n\n\n\n\n\n\n\nmask_6step = GSandBinarise(im_diff_normabs6, thresh_method='manual', thresh=20)\nmask_6step\n\n\n\n\n\n\n\n\nRepeat the same for other num_inference_steps=8 values to get a better mask. Also remember to play around with manual threshold value, to get ideal mask. \nThe mask output looked decent for num_inference_steps=8 in the single-output section, so let‚Äôs try that here:\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\"\nimages_r8 = pipe(prompt=prompt, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=8).images\nimage_grid(images_r8, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\"\nimages_q8 = pipe(prompt=prompt_q, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=8).images\nimage_grid(images_q8, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\nav_image_r8 = get_averageIm(images_r8)\nav_image_r8\n\n\n\n\n\n\n\n\n\nav_image_q8 = get_averageIm(images_q8)\nav_image_q8\n\n\n\n\n\n\n\n\n\nim_diff_normabs8 = norm_diff_abs(av_image_r8, av_image_q8)\nmask_otsu_8step = GSandBinarise(im_diff_normabs8, thresh_method='OTSU')\nmask_otsu_8step\n\n\n\n\n\n\n\n\n\nmask_8step = GSandBinarise(im_diff_normabs8, thresh_method='manual', thresh=25)\nmask_8step\n\n\n\n\n\n\n\n\nThat looks better than the previous mask. We‚Äôve considerably reduced the white patches in the background. Let‚Äôs try num_inference_steps=7.\n\n#inf steps 7\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\"\nimages_r7 = pipe(prompt=prompt, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=7).images\nimage_grid(images_r7, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\"\nimages_q7 = pipe(prompt=prompt_q, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=7).images\nimage_grid(images_q7, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\nav_image_r7 = get_averageIm(images_r7)\nav_image_r7\n\n\n\n\n\n\n\n\n\nav_image_q7 = get_averageIm(images_q7)\nav_image_q7\n\n\n\n\n\n\n\n\n\nim_diff_normabs7 = norm_diff_abs(av_image_r7, av_image_q7)\nmask_otsu_7step = GSandBinarise(im_diff_normabs7, thresh_method='OTSU')\nmask_otsu_7step\n\n\n\n\n\n\n\n\n\nmask_7step = GSandBinarise(im_diff_normabs7, thresh_method='manual', thresh=25)\nmask_7step\n\n\n\n\n\n\n\n\nThis mask (num_inference_steps=7) has a slightly cleaner outline than the one for num_inference_steps=8. Let‚Äôs try going a bit higher; num_inference_steps=10\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\"\nimages_r10 = pipe(prompt=prompt, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=10).images\nimage_grid(images_r10, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\"\nimages_q10 = pipe(prompt=prompt_q, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=10).images\nimage_grid(images_q10, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\nAs one can see, there are a few weird images, and all the images are quite deformed with respect to the original init_image. Let‚Äôs look at the generated mask.\n\nav_image_r10 = get_averageIm(images_r10)\nav_image_r10\n\nav_image_q10 = get_averageIm(images_q10)\nav_image_q10\n\n\n\n\n\n\n\n\n\nim_diff_normabs10 = norm_diff_abs(av_image_r10, av_image_q10)\nmask_otsu_10step = GSandBinarise(im_diff_normabs10, thresh_method='OTSU')\nmask_otsu_10step\n\n\n\n\n\n\n\n\n\nmask_10step = GSandBinarise(im_diff_normabs10, thresh_method='manual', thresh=25)\nmask_10step\n\n\n\n\n\n\n\n\nWe start to see a more patchy outline, because the denoised images have deviated more and more from the original outline. Let‚Äôs go down one step and try num_inference_steps=9.\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\"\nimages_r9 = pipe(prompt=prompt, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=9).images\nimage_grid(images_r9, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\"\nimages_q9 = pipe(prompt=prompt_q, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=9).images\nimage_grid(images_q9, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\nThe images are still weird and vary considerably in the position of the subject, so we can expect the mask to be ‚Äúnoisy‚Äù as well.\n\nav_image_r9 = get_averageIm(images_r9)\nav_image_r9\n\n\n\n\n\n\n\n\n\nav_image_q9 = get_averageIm(images_q9)\nav_image_q9\n\n\n\n\n\n\n\n\n\nim_diff_normabs9 = norm_diff_abs(av_image_r9, av_image_q9)\nmask_otsu_9step = GSandBinarise(im_diff_normabs9, thresh_method='OTSU')\nmask_otsu_9step\n\n\n\n\n\n\n\n\n\nmask_9step = GSandBinarise(im_diff_normabs9, thresh_method='manual', thresh=30)\nmask_9step\n\n\n\n\n\n\n\n\nThough better than the previous run, this still has a rather unclear outline. From the above analyses, we can pick num_inference_steps=6, 7, 8 as yielding usable pixel masks.\n\nmask_list = [mask_6step, mask_7step, mask_8step]\nimage_grid(mask_list, rows=1, cols=3)\n\n\n\n\n\n\n\n\nIf we could see how these masks look on the original image, I suppose they‚Äôd be more convincing. I‚Äôm currently facing a minor bug in overlaying a red mask onto the original image; and this notebook will be updated with the same once that is resolved.\nThough a far cry from the clean mask illustrated in the DiffEdit paper, I suppose this isn‚Äôt a bad start. As I experiment with other techniques (like using other noise schedulers) to generate a fully clean mask, I‚Äôll be adding relevant updates on this post. \nHopefully you got something from your time here! If you read this and have any suggestions/comments on how to improve my code (or words!), please reach out to me @Twitter. \nCheers!! üòÑ"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "ML Algorithms Core Member (2023 - Current)\nDeveloping multimodal models for battlefield triage, using NVIDIA Jetson Orin."
  },
  {
    "objectID": "projects.html#darpa-triage-challenge",
    "href": "projects.html#darpa-triage-challenge",
    "title": "Projects",
    "section": "",
    "text": "ML Algorithms Core Member (2023 - Current)\nDeveloping multimodal models for battlefield triage, using NVIDIA Jetson Orin."
  },
  {
    "objectID": "projects.html#context-aware-imitation-learning",
    "href": "projects.html#context-aware-imitation-learning",
    "title": "Projects",
    "section": "Context Aware Imitation Learning",
    "text": "Context Aware Imitation Learning\nResearcher (2024 - Current)\nImproving success rates in long-horizon imitation learning with language-conditioned diffusion models."
  },
  {
    "objectID": "projects.html#machine-unlearning",
    "href": "projects.html#machine-unlearning",
    "title": "Projects",
    "section": "Machine Unlearning",
    "text": "Machine Unlearning\nResearcher (2023 - Current)\nDeveloped a novel approach integrating importance-weighted distilled images for secure data removal."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hi there!",
    "section": "",
    "text": "Playing around with code and words; in a constant state of figuring üîÆüé∂"
  }
]